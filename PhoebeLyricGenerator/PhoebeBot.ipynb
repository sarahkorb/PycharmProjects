{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ac0228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3ea1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape website for lyrics \n",
    "page = requests.get('https://www.letras.com/phoebe-bridgers/')\n",
    "soup = BeautifulSoup(page.text,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f72ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfSongs = []\n",
    "\n",
    "for i in soup.find_all('li'):\n",
    "  listOfSongs.append(i)\n",
    "\n",
    "listOfSongs = [str(x) for x in listOfSongs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e03d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSongs = []\n",
    "\n",
    "for link in listOfSongs:\n",
    "  if 'https' in link:\n",
    "    newSongs.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d750a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li><a class=\"user-links-link\" href=\"https://accounts.letras.com/editar/\" rel=\"noopener\" target=\"_blank\">Editar</a></li>\n",
      "<li><a class=\"user-links-link\" href=\"https://accounts.letras.com/logout/\" id=\"js-logout\">Salir</a></li>\n",
      "<li><a href=\"/bad-bunny/\"> <img alt=\"\" height=\"76\" src=\"https://akamai.sscdn.co/uploadfile/letras/fotos/c/e/3/c/ce3c629b492833e0dc7a268a8c5c3454-tb5.jpg\" srcset=\"https://akamai.sscdn.co/uploadfile/letras/fotos/c/e/3/c/ce3c629b492833e0dc7a268a8c5c3454-tb5.jpg 1x, https://akamai.sscdn.co/uploadfile/letras/fotos/c/e/3/c/ce3c629b492833e0dc7a268a8c5c3454-tb_180.jpg 2x\" width=\"76\"> <b>Bad Bunny</b> </img></a></li>\n",
      "<li><a href=\"/taylor-swift/\"> <img alt=\"\" height=\"76\" src=\"https://akamai.sscdn.co/uploadfile/letras/fotos/8/e/f/8/8ef84fdd1ce44bffbe0463a86097b28b-tb5.jpg\" srcset=\"https://akamai.sscdn.co/uploadfile/letras/fotos/8/e/f/8/8ef84fdd1ce44bffbe0463a86097b28b-tb5.jpg 1x, https://akamai.sscdn.co/uploadfile/letras/fotos/8/e/f/8/8ef84fdd1ce44bffbe0463a86097b28b-tb_180.jpg 2x\" width=\"76\"> <b>Taylor Swift</b> </img></a></li>\n",
      "<li><a href=\"/the-weeknd/\"> <img alt=\"\" height=\"76\" src=\"https://akamai.sscdn.co/uploadfile/letras/fotos/8/9/a/1/89a114a2f4a89231c5508efb3b700434-tb5.jpg\" srcset=\"https://akamai.sscdn.co/uploadfile/letras/fotos/8/9/a/1/89a114a2f4a89231c5508efb3b700434-tb5.jpg 1x, https://akamai.sscdn.co/uploadfile/letras/fotos/8/9/a/1/89a114a2f4a89231c5508efb3b700434-tb_180.jpg 2x\" width=\"76\"> <b>The Weeknd</b> </img></a></li>\n"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for song in newSongs:\n",
    "  try:\n",
    "    urls.append(song.split('\"')[13])\n",
    "  except:\n",
    "    print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6efdcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "file = io.open('PhoebeSongs.txt', 'w')\n",
    "lyrics = []\n",
    "for url in urls:\n",
    "    if url == 'true':\n",
    "        continue\n",
    "    else:\n",
    "        page = requests.get(url)  \n",
    "        soup = BeautifulSoup(page.text,'html.parser')\n",
    "        for br in soup.find('article').find_all('br'):\n",
    "            br.replace_with(\"\\n\" + br.text)\n",
    "        #<p> tag wraps lyrics\n",
    "        for lyric in soup.find('article').find_all('p'):\n",
    "            lyric = str(lyric).replace(\"<br/>\", \"\\s\").replace(\"</p>\", \"\").replace(\"<p>\", \"\").replace(\"!\", \"\")\n",
    "            lyrics.append(lyric)\n",
    "\n",
    "\n",
    "for lyric in lyrics:\n",
    "    file.write(lyric)\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "986d2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -qq lightning==1.8.0 transformers==4.21.3 aitextgen==0.6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8604f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "file_name = \"PhoebeSongs.txt\"\n",
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "481255f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2ConfigCPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbe5b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = aitextgen(tokenizer_file=tokenizer_file, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e6a33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1591/1591 [00:00<00:00, 75646.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenDataset containing 22,257 subsets loaded from file at PhoebeSongs.txt."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TokenDataset(file_name, tokenizer_file=tokenizer_file, block_size=64)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ab6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2023-10-25 19:48:18,416] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-25 19:48:18,416] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-25 19:48:18,416] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-25 19:48:18,416] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|▏                                        | 20/5000 [00:11<47:32,  1.75it/s]\u001b[A\n",
      "Loss: 0.611 — Avg: 0.564:   0%|               | 20/5000 [00:11<47:42,  1.74it/s]\u001b[A\n",
      "Loss: 0.611 — Avg: 0.564:   1%|               | 40/5000 [00:18<39:10,  2.11it/s]\u001b[A\n",
      "Loss: 0.707 — Avg: 0.581:   1%|               | 40/5000 [00:18<39:12,  2.11it/s]\u001b[A\n",
      "Loss: 0.707 — Avg: 0.581:   1%|▏              | 60/5000 [00:26<35:59,  2.29it/s]\u001b[A\n",
      "Loss: 0.727 — Avg: 0.610:   1%|▏              | 60/5000 [00:26<36:00,  2.29it/s]\u001b[A\n",
      "Loss: 0.727 — Avg: 0.610:   2%|▏              | 80/5000 [00:34<35:17,  2.32it/s]\u001b[A\n",
      "Loss: 0.745 — Avg: 0.629:   2%|▏              | 80/5000 [00:34<35:17,  2.32it/s]\u001b[A\n",
      "Loss: 0.745 — Avg: 0.629:   2%|▎             | 100/5000 [00:42<34:28,  2.37it/s]\u001b[A\n",
      "Loss: 0.693 — Avg: 0.649:   2%|▎             | 100/5000 [00:42<34:29,  2.37it/s]\u001b[A\n",
      "Loss: 0.693 — Avg: 0.649:   2%|▎             | 120/5000 [00:50<33:55,  2.40it/s]\u001b[A\n",
      "Loss: 0.615 — Avg: 0.650:   2%|▎             | 120/5000 [00:50<33:55,  2.40it/s]\u001b[A\n",
      "Loss: 0.615 — Avg: 0.650:   3%|▍             | 140/5000 [00:57<33:20,  2.43it/s]\u001b[A\n",
      "Loss: 0.747 — Avg: 0.659:   3%|▍             | 140/5000 [00:57<33:20,  2.43it/s]\u001b[A\n",
      "Loss: 0.747 — Avg: 0.659:   3%|▍             | 160/5000 [01:04<32:37,  2.47it/s]\u001b[A\n",
      "Loss: 0.620 — Avg: 0.659:   3%|▍             | 160/5000 [01:04<32:37,  2.47it/s]\u001b[A\n",
      "Loss: 0.620 — Avg: 0.659:   4%|▌             | 180/5000 [01:12<32:13,  2.49it/s]\u001b[A\n",
      "Loss: 0.672 — Avg: 0.660:   4%|▌             | 180/5000 [01:12<32:14,  2.49it/s]\u001b[A\n",
      "Loss: 0.672 — Avg: 0.660:   4%|▌             | 200/5000 [01:19<31:55,  2.51it/s]\u001b[A\n",
      "Loss: 0.693 — Avg: 0.662:   4%|▌             | 200/5000 [01:19<31:56,  2.51it/s]\u001b[A\n",
      "Loss: 0.693 — Avg: 0.662:   4%|▌             | 220/5000 [01:30<32:38,  2.44it/s]\u001b[A\n",
      "Loss: 0.783 — Avg: 0.678:   4%|▌             | 220/5000 [01:30<32:38,  2.44it/s]\u001b[A\n",
      "Loss: 0.783 — Avg: 0.678:   5%|▋             | 240/5000 [01:37<32:12,  2.46it/s]\u001b[A\n",
      "Loss: 0.689 — Avg: 0.688:   5%|▋             | 240/5000 [01:37<32:13,  2.46it/s]\u001b[A\n",
      "Loss: 0.689 — Avg: 0.688:   5%|▋             | 260/5000 [01:45<32:01,  2.47it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.683:   5%|▋             | 260/5000 [01:45<32:01,  2.47it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.683:   6%|▊             | 280/5000 [01:53<31:47,  2.47it/s]\u001b[A\n",
      "Loss: 0.699 — Avg: 0.681:   6%|▊             | 280/5000 [01:53<31:48,  2.47it/s]\u001b[A\n",
      "Loss: 0.699 — Avg: 0.681:   6%|▊             | 300/5000 [02:01<31:47,  2.46it/s]\u001b[A\n",
      "Loss: 0.690 — Avg: 0.683:   6%|▊             | 300/5000 [02:01<31:47,  2.46it/s]\u001b[A\n",
      "Loss: 0.690 — Avg: 0.683:   6%|▉             | 320/5000 [02:08<31:24,  2.48it/s]\u001b[A\n",
      "Loss: 0.639 — Avg: 0.678:   6%|▉             | 320/5000 [02:08<31:24,  2.48it/s]\u001b[A\n",
      "Loss: 0.639 — Avg: 0.678:   7%|▉             | 340/5000 [02:16<31:15,  2.49it/s]\u001b[A\n",
      "Loss: 0.669 — Avg: 0.677:   7%|▉             | 340/5000 [02:16<31:15,  2.48it/s]\u001b[A\n",
      "Loss: 0.669 — Avg: 0.677:   7%|█             | 360/5000 [02:25<31:10,  2.48it/s]\u001b[A\n",
      "Loss: 0.661 — Avg: 0.672:   7%|█             | 360/5000 [02:25<31:11,  2.48it/s]\u001b[A\n",
      "Loss: 0.661 — Avg: 0.672:   8%|█             | 380/5000 [02:31<30:44,  2.50it/s]\u001b[A\n",
      "Loss: 0.724 — Avg: 0.678:   8%|█             | 380/5000 [02:31<30:45,  2.50it/s]\u001b[A\n",
      "Loss: 0.724 — Avg: 0.678:   8%|█             | 400/5000 [02:38<30:17,  2.53it/s]\u001b[A\n",
      "Loss: 0.682 — Avg: 0.685:   8%|█             | 400/5000 [02:38<30:17,  2.53it/s]\u001b[A\n",
      "Loss: 0.682 — Avg: 0.685:   8%|█▏            | 420/5000 [02:45<30:02,  2.54it/s]\u001b[A\n",
      "Loss: 0.680 — Avg: 0.684:   8%|█▏            | 420/5000 [02:45<30:02,  2.54it/s]\u001b[A\n",
      "Loss: 0.680 — Avg: 0.684:   9%|█▏            | 440/5000 [02:53<29:58,  2.54it/s]\u001b[A\n",
      "Loss: 0.646 — Avg: 0.676:   9%|█▏            | 440/5000 [02:53<29:58,  2.54it/s]\u001b[A\n",
      "Loss: 0.646 — Avg: 0.676:   9%|█▎            | 460/5000 [03:02<30:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.720 — Avg: 0.684:   9%|█▎            | 460/5000 [03:02<30:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.720 — Avg: 0.684:  10%|█▎            | 480/5000 [03:10<29:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.707 — Avg: 0.686:  10%|█▎            | 480/5000 [03:10<29:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.707 — Avg: 0.686:  10%|█▍            | 500/5000 [03:18<29:47,  2.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m500 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [47:46<11:48:12,  1.10it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m500 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [47:46<11:48:13,  1.10it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [47:46<11:48:15,  1.10it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "And I'll say the same things I've been talking to light preir\n",
      "Waiting right for the last timeBut I'm do\n",
      "We surprised by what I'd do for love\n",
      "Some things I'd never expect\n",
      "They killed a fan\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [47:46<11:48:15,  1.10it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [47:46<11:48:15,  1.10it/s]\n",
      "Loss: 0.676 — Avg: 0.684:  10%|█▍            | 500/5000 [03:18<29:49,  2.51it/s]\u001b[A\n",
      "Loss: 0.676 — Avg: 0.684:  10%|█▍            | 520/5000 [03:27<29:43,  2.51it/s]\u001b[A\n",
      "Loss: 0.635 — Avg: 0.675:  10%|█▍            | 520/5000 [03:27<29:43,  2.51it/s]\u001b[A\n",
      "Loss: 0.635 — Avg: 0.675:  11%|█▌            | 540/5000 [03:35<29:39,  2.51it/s]\u001b[A\n",
      "Loss: 0.650 — Avg: 0.674:  11%|█▌            | 540/5000 [03:35<29:39,  2.51it/s]\u001b[A\n",
      "Loss: 0.650 — Avg: 0.674:  11%|█▌            | 560/5000 [03:45<29:46,  2.49it/s]\u001b[A\n",
      "Loss: 0.682 — Avg: 0.670:  11%|█▌            | 560/5000 [03:45<29:46,  2.49it/s]\u001b[A\n",
      "Loss: 0.682 — Avg: 0.670:  12%|█▌            | 580/5000 [03:53<29:40,  2.48it/s]\u001b[A\n",
      "Loss: 0.645 — Avg: 0.671:  12%|█▌            | 580/5000 [03:53<29:40,  2.48it/s]\u001b[A\n",
      "Loss: 0.645 — Avg: 0.671:  12%|█▌            | 580/5000 [04:06<31:21,  2.35it/s]\u001b[A\n",
      "Loss: 0.645 — Avg: 0.671:  12%|█▋            | 600/5000 [04:07<30:12,  2.43it/s]\u001b[A\n",
      "Loss: 0.633 — Avg: 0.665:  12%|█▋            | 600/5000 [04:07<30:12,  2.43it/s]\u001b[A\n",
      "Loss: 0.633 — Avg: 0.665:  12%|█▋            | 620/5000 [04:15<30:03,  2.43it/s]\u001b[A\n",
      "Loss: 0.692 — Avg: 0.669:  12%|█▋            | 620/5000 [04:15<30:03,  2.43it/s]\u001b[A\n",
      "Loss: 0.692 — Avg: 0.669:  13%|█▊            | 640/5000 [04:22<29:48,  2.44it/s]\u001b[A\n",
      "Loss: 0.613 — Avg: 0.663:  13%|█▊            | 640/5000 [04:22<29:48,  2.44it/s]\u001b[A\n",
      "Loss: 0.613 — Avg: 0.663:  13%|█▊            | 660/5000 [04:30<29:36,  2.44it/s]\u001b[A\n",
      "Loss: 0.664 — Avg: 0.659:  13%|█▊            | 660/5000 [04:30<29:36,  2.44it/s]\u001b[A\n",
      "Loss: 0.664 — Avg: 0.659:  14%|█▉            | 680/5000 [04:37<29:21,  2.45it/s]\u001b[A\n",
      "Loss: 0.620 — Avg: 0.655:  14%|█▉            | 680/5000 [04:37<29:21,  2.45it/s]\u001b[A\n",
      "Loss: 0.620 — Avg: 0.655:  14%|█▉            | 700/5000 [04:44<29:09,  2.46it/s]\u001b[A\n",
      "Loss: 0.653 — Avg: 0.654:  14%|█▉            | 700/5000 [04:44<29:09,  2.46it/s]\u001b[A\n",
      "Loss: 0.653 — Avg: 0.654:  14%|██            | 720/5000 [04:52<28:58,  2.46it/s]\u001b[A\n",
      "Loss: 0.659 — Avg: 0.652:  14%|██            | 720/5000 [04:52<28:58,  2.46it/s]\u001b[A\n",
      "Loss: 0.659 — Avg: 0.652:  15%|██            | 740/5000 [04:59<28:43,  2.47it/s]\u001b[A\n",
      "Loss: 0.692 — Avg: 0.657:  15%|██            | 740/5000 [04:59<28:43,  2.47it/s]\u001b[A\n",
      "Loss: 0.692 — Avg: 0.657:  15%|██▏           | 760/5000 [05:07<28:34,  2.47it/s]\u001b[A\n",
      "Loss: 0.633 — Avg: 0.654:  15%|██▏           | 760/5000 [05:07<28:34,  2.47it/s]\u001b[A\n",
      "Loss: 0.633 — Avg: 0.654:  16%|██▏           | 780/5000 [05:15<28:24,  2.48it/s]\u001b[A\n",
      "Loss: 0.639 — Avg: 0.655:  16%|██▏           | 780/5000 [05:15<28:24,  2.48it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.639 — Avg: 0.655:  16%|██▏           | 800/5000 [05:22<28:12,  2.48it/s]\u001b[A\n",
      "Loss: 0.694 — Avg: 0.660:  16%|██▏           | 800/5000 [05:22<28:12,  2.48it/s]\u001b[A\n",
      "Loss: 0.694 — Avg: 0.660:  16%|██▎           | 820/5000 [05:29<28:00,  2.49it/s]\u001b[A\n",
      "Loss: 0.625 — Avg: 0.657:  16%|██▎           | 820/5000 [05:29<28:00,  2.49it/s]\u001b[A\n",
      "Loss: 0.625 — Avg: 0.657:  17%|██▎           | 840/5000 [05:38<27:56,  2.48it/s]\u001b[A\n",
      "Loss: 0.604 — Avg: 0.648:  17%|██▎           | 840/5000 [05:38<27:56,  2.48it/s]\u001b[A\n",
      "Loss: 0.604 — Avg: 0.648:  17%|██▍           | 860/5000 [05:46<27:48,  2.48it/s]\u001b[A\n",
      "Loss: 0.646 — Avg: 0.646:  17%|██▍           | 860/5000 [05:46<27:48,  2.48it/s]\u001b[A\n",
      "Loss: 0.646 — Avg: 0.646:  18%|██▍           | 880/5000 [05:55<27:42,  2.48it/s]\u001b[A\n",
      "Loss: 0.628 — Avg: 0.644:  18%|██▍           | 880/5000 [05:55<27:42,  2.48it/s]\u001b[A\n",
      "Loss: 0.628 — Avg: 0.644:  18%|██▌           | 900/5000 [06:04<27:38,  2.47it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.642:  18%|██▌           | 900/5000 [06:04<27:38,  2.47it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.642:  18%|██▌           | 920/5000 [06:11<27:26,  2.48it/s]\u001b[A\n",
      "Loss: 0.650 — Avg: 0.646:  18%|██▌           | 920/5000 [06:11<27:26,  2.48it/s]\u001b[A\n",
      "Loss: 0.650 — Avg: 0.646:  19%|██▋           | 940/5000 [06:19<27:17,  2.48it/s]\u001b[A\n",
      "Loss: 0.697 — Avg: 0.648:  19%|██▋           | 940/5000 [06:19<27:17,  2.48it/s]\u001b[A\n",
      "Loss: 0.697 — Avg: 0.648:  19%|██▋           | 960/5000 [06:28<27:14,  2.47it/s]\u001b[A\n",
      "Loss: 0.629 — Avg: 0.652:  19%|██▋           | 960/5000 [06:28<27:14,  2.47it/s]\u001b[A\n",
      "Loss: 0.629 — Avg: 0.652:  20%|██▋           | 980/5000 [06:36<27:04,  2.47it/s]\u001b[A\n",
      "Loss: 0.635 — Avg: 0.650:  20%|██▋           | 980/5000 [06:36<27:04,  2.47it/s]\u001b[A\n",
      "Loss: 0.635 — Avg: 0.650:  20%|██▌          | 1000/5000 [06:43<26:54,  2.48it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [51:11<12:38:51,  1.03it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [51:11<12:38:51,  1.03it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [51:11<12:38:54,  1.03it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "Long before, Mallow at the quoke down the  Englood drain\n",
      "The trash out on the being line\n",
      "He'll come home, I won't believe\n",
      "But I know, I knowDriving out on the back down\n",
      "We find a string,\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [51:11<12:38:54,  1.03it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [51:11<12:38:54,  1.03it/s]\n",
      "Loss: 0.585 — Avg: 0.640:  20%|██▌          | 1000/5000 [06:43<26:55,  2.48it/s]\u001b[A\n",
      "Loss: 0.585 — Avg: 0.640:  20%|██▋          | 1020/5000 [06:50<26:42,  2.48it/s]\u001b[A\n",
      "Loss: 0.622 — Avg: 0.633:  20%|██▋          | 1020/5000 [06:50<26:42,  2.48it/s]\u001b[A\n",
      "Loss: 0.622 — Avg: 0.633:  21%|██▋          | 1040/5000 [06:57<26:27,  2.49it/s]\u001b[A\n",
      "Loss: 0.641 — Avg: 0.631:  21%|██▋          | 1040/5000 [06:57<26:27,  2.49it/s]\u001b[A\n",
      "Loss: 0.641 — Avg: 0.631:  21%|██▊          | 1060/5000 [07:04<26:19,  2.49it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.637:  21%|██▊          | 1060/5000 [07:04<26:19,  2.49it/s]\u001b[A\n",
      "Loss: 0.656 — Avg: 0.637:  22%|██▊          | 1080/5000 [07:12<26:10,  2.50it/s]\u001b[A\n",
      "Loss: 0.623 — Avg: 0.636:  22%|██▊          | 1080/5000 [07:12<26:10,  2.50it/s]\u001b[A\n",
      "Loss: 0.623 — Avg: 0.636:  22%|██▊          | 1100/5000 [07:19<25:59,  2.50it/s]\u001b[A\n",
      "Loss: 0.598 — Avg: 0.634:  22%|██▊          | 1100/5000 [07:19<25:59,  2.50it/s]\u001b[A\n",
      "Loss: 0.598 — Avg: 0.634:  22%|██▉          | 1120/5000 [07:27<25:49,  2.50it/s]\u001b[A\n",
      "Loss: 0.636 — Avg: 0.631:  22%|██▉          | 1120/5000 [07:27<25:49,  2.50it/s]\u001b[A\n",
      "Loss: 0.636 — Avg: 0.631:  23%|██▉          | 1140/5000 [07:34<25:38,  2.51it/s]\u001b[A\n",
      "Loss: 0.621 — Avg: 0.629:  23%|██▉          | 1140/5000 [07:34<25:38,  2.51it/s]\u001b[A\n",
      "Loss: 0.621 — Avg: 0.629:  23%|███          | 1160/5000 [07:42<25:30,  2.51it/s]\u001b[A\n",
      "Loss: 0.617 — Avg: 0.627:  23%|███          | 1160/5000 [07:42<25:30,  2.51it/s]\u001b[A\n",
      "Loss: 0.617 — Avg: 0.627:  24%|███          | 1180/5000 [07:49<25:20,  2.51it/s]\u001b[A\n",
      "Loss: 0.641 — Avg: 0.623:  24%|███          | 1180/5000 [07:49<25:20,  2.51it/s]\u001b[A\n",
      "Loss: 0.641 — Avg: 0.623:  24%|███          | 1200/5000 [07:57<25:12,  2.51it/s]\u001b[A\n",
      "Loss: 0.652 — Avg: 0.632:  24%|███          | 1200/5000 [07:57<25:12,  2.51it/s]\u001b[A\n",
      "Loss: 0.652 — Avg: 0.632:  24%|███▏         | 1220/5000 [08:04<25:02,  2.52it/s]\u001b[A\n",
      "Loss: 0.603 — Avg: 0.629:  24%|███▏         | 1220/5000 [08:04<25:02,  2.52it/s]\u001b[A\n",
      "Loss: 0.603 — Avg: 0.629:  25%|███▏         | 1240/5000 [08:13<24:57,  2.51it/s]\u001b[A\n",
      "Loss: 0.627 — Avg: 0.626:  25%|███▏         | 1240/5000 [08:13<24:57,  2.51it/s]\u001b[A\n",
      "Loss: 0.627 — Avg: 0.626:  25%|███▎         | 1260/5000 [08:22<24:50,  2.51it/s]\u001b[A\n",
      "Loss: 0.572 — Avg: 0.625:  25%|███▎         | 1260/5000 [08:22<24:50,  2.51it/s]\u001b[A\n",
      "Loss: 0.572 — Avg: 0.625:  26%|███▎         | 1280/5000 [08:29<24:41,  2.51it/s]\u001b[A\n",
      "Loss: 0.565 — Avg: 0.613:  26%|███▎         | 1280/5000 [08:29<24:41,  2.51it/s]\u001b[A\n",
      "Loss: 0.565 — Avg: 0.613:  26%|███▍         | 1300/5000 [08:36<24:31,  2.51it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.605:  26%|███▍         | 1300/5000 [08:37<24:31,  2.51it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.605:  26%|███▍         | 1320/5000 [08:44<24:21,  2.52it/s]\u001b[A\n",
      "Loss: 0.610 — Avg: 0.602:  26%|███▍         | 1320/5000 [08:44<24:21,  2.52it/s]\u001b[A\n",
      "Loss: 0.610 — Avg: 0.602:  27%|███▍         | 1340/5000 [08:51<24:12,  2.52it/s]\u001b[A\n",
      "Loss: 0.605 — Avg: 0.603:  27%|███▍         | 1340/5000 [08:51<24:12,  2.52it/s]\u001b[A\n",
      "Loss: 0.605 — Avg: 0.603:  27%|███▌         | 1360/5000 [08:59<24:03,  2.52it/s]\u001b[A\n",
      "Loss: 0.595 — Avg: 0.600:  27%|███▌         | 1360/5000 [08:59<24:03,  2.52it/s]\u001b[A\n",
      "Loss: 0.595 — Avg: 0.600:  28%|███▌         | 1380/5000 [09:04<23:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.637 — Avg: 0.604:  28%|███▌         | 1380/5000 [09:04<23:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.637 — Avg: 0.604:  28%|███▋         | 1400/5000 [09:12<23:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.608 — Avg: 0.606:  28%|███▋         | 1400/5000 [09:12<23:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.608 — Avg: 0.606:  28%|███▋         | 1420/5000 [09:20<23:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.573 — Avg: 0.602:  28%|███▋         | 1420/5000 [09:20<23:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.573 — Avg: 0.602:  29%|███▋         | 1440/5000 [09:26<23:21,  2.54it/s]\u001b[A\n",
      "Loss: 0.617 — Avg: 0.600:  29%|███▋         | 1440/5000 [09:27<23:21,  2.54it/s]\u001b[A\n",
      "Loss: 0.617 — Avg: 0.600:  29%|███▊         | 1460/5000 [09:34<23:12,  2.54it/s]\u001b[A\n",
      "Loss: 0.579 — Avg: 0.601:  29%|███▊         | 1460/5000 [09:34<23:12,  2.54it/s]\u001b[A\n",
      "Loss: 0.579 — Avg: 0.601:  30%|███▊         | 1480/5000 [09:42<23:04,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.594:  30%|███▊         | 1480/5000 [09:42<23:04,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.594:  30%|███▉         | 1500/5000 [09:50<22:58,  2.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m1,500 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [54:18<13:25:06,  1.03s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m1,500 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [54:18<13:25:06,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [54:19<13:25:08,  1.03s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "Laying there the lifeble\n",
      "As, our famp and friends\n",
      "And when you're do you're bigning\n",
      "If you're a taller be anything you're on the end\n",
      "We find our way out\n",
      "Of a suicide pact of\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [54:19<13:25:08,  1.03s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [54:19<13:25:08,  1.03s/it]\n",
      "Loss: 0.624 — Avg: 0.595:  30%|███▉         | 1500/5000 [09:51<22:59,  2.54it/s]\u001b[A\n",
      "Loss: 0.624 — Avg: 0.595:  30%|███▉         | 1520/5000 [09:59<22:52,  2.54it/s]\u001b[A\n",
      "Loss: 0.556 — Avg: 0.591:  30%|███▉         | 1520/5000 [09:59<22:52,  2.54it/s]\u001b[A\n",
      "Loss: 0.556 — Avg: 0.591:  31%|████         | 1540/5000 [10:07<22:44,  2.54it/s]\u001b[A\n",
      "Loss: 0.601 — Avg: 0.589:  31%|████         | 1540/5000 [10:07<22:44,  2.54it/s]\u001b[A\n",
      "Loss: 0.601 — Avg: 0.589:  31%|████         | 1560/5000 [10:14<22:35,  2.54it/s]\u001b[A\n",
      "Loss: 0.626 — Avg: 0.595:  31%|████         | 1560/5000 [10:14<22:35,  2.54it/s]\u001b[A\n",
      "Loss: 0.626 — Avg: 0.595:  32%|████         | 1580/5000 [10:22<22:28,  2.54it/s]\u001b[A\n",
      "Loss: 0.557 — Avg: 0.592:  32%|████         | 1580/5000 [10:22<22:28,  2.54it/s]\u001b[A\n",
      "Loss: 0.557 — Avg: 0.592:  32%|████▏        | 1600/5000 [10:31<22:21,  2.53it/s]\u001b[A\n",
      "Loss: 0.561 — Avg: 0.586:  32%|████▏        | 1600/5000 [10:31<22:22,  2.53it/s]\u001b[A\n",
      "Loss: 0.561 — Avg: 0.586:  32%|████▏        | 1620/5000 [10:39<22:14,  2.53it/s]\u001b[A\n",
      "Loss: 0.584 — Avg: 0.584:  32%|████▏        | 1620/5000 [10:39<22:14,  2.53it/s]\u001b[A\n",
      "Loss: 0.584 — Avg: 0.584:  33%|████▎        | 1640/5000 [10:48<22:07,  2.53it/s]\u001b[A\n",
      "Loss: 0.576 — Avg: 0.587:  33%|████▎        | 1640/5000 [10:48<22:07,  2.53it/s]\u001b[A\n",
      "Loss: 0.576 — Avg: 0.587:  33%|████▎        | 1660/5000 [10:56<22:00,  2.53it/s]\u001b[A\n",
      "Loss: 0.579 — Avg: 0.581:  33%|████▎        | 1660/5000 [10:56<22:00,  2.53it/s]\u001b[A\n",
      "Loss: 0.579 — Avg: 0.581:  34%|████▎        | 1680/5000 [11:03<21:51,  2.53it/s]\u001b[A\n",
      "Loss: 0.581 — Avg: 0.587:  34%|████▎        | 1680/5000 [11:03<21:51,  2.53it/s]\u001b[A\n",
      "Loss: 0.581 — Avg: 0.587:  34%|████▍        | 1700/5000 [11:10<21:41,  2.54it/s]\u001b[A\n",
      "Loss: 0.564 — Avg: 0.577:  34%|████▍        | 1700/5000 [11:10<21:41,  2.54it/s]\u001b[A\n",
      "Loss: 0.564 — Avg: 0.577:  34%|████▍        | 1720/5000 [11:18<21:33,  2.54it/s]\u001b[A\n",
      "Loss: 0.595 — Avg: 0.582:  34%|████▍        | 1720/5000 [11:18<21:33,  2.54it/s]\u001b[A\n",
      "Loss: 0.595 — Avg: 0.582:  35%|████▌        | 1740/5000 [11:24<21:23,  2.54it/s]\u001b[A\n",
      "Loss: 0.539 — Avg: 0.577:  35%|████▌        | 1740/5000 [11:25<21:23,  2.54it/s]\u001b[A\n",
      "Loss: 0.539 — Avg: 0.577:  35%|████▌        | 1760/5000 [11:33<21:16,  2.54it/s]\u001b[A\n",
      "Loss: 0.632 — Avg: 0.584:  35%|████▌        | 1760/5000 [11:33<21:16,  2.54it/s]\u001b[A\n",
      "Loss: 0.632 — Avg: 0.584:  36%|████▋        | 1780/5000 [11:41<21:08,  2.54it/s]\u001b[A\n",
      "Loss: 0.510 — Avg: 0.578:  36%|████▋        | 1780/5000 [11:41<21:08,  2.54it/s]\u001b[A\n",
      "Loss: 0.510 — Avg: 0.578:  36%|████▋        | 1800/5000 [11:48<21:00,  2.54it/s]\u001b[A\n",
      "Loss: 0.551 — Avg: 0.570:  36%|████▋        | 1800/5000 [11:48<21:00,  2.54it/s]\u001b[A\n",
      "Loss: 0.551 — Avg: 0.570:  36%|████▋        | 1820/5000 [11:56<20:52,  2.54it/s]\u001b[A\n",
      "Loss: 0.599 — Avg: 0.570:  36%|████▋        | 1820/5000 [11:56<20:52,  2.54it/s]\u001b[A\n",
      "Loss: 0.599 — Avg: 0.570:  37%|████▊        | 1840/5000 [12:05<20:46,  2.53it/s]\u001b[A\n",
      "Loss: 0.645 — Avg: 0.581:  37%|████▊        | 1840/5000 [12:05<20:46,  2.53it/s]\u001b[A\n",
      "Loss: 0.645 — Avg: 0.581:  37%|████▊        | 1860/5000 [12:14<20:39,  2.53it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.581:  37%|████▊        | 1860/5000 [12:14<20:40,  2.53it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.581:  38%|████▉        | 1880/5000 [12:21<20:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.579:  38%|████▉        | 1880/5000 [12:21<20:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.579:  38%|████▉        | 1900/5000 [12:30<20:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.574 — Avg: 0.576:  38%|████▉        | 1900/5000 [12:30<20:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.574 — Avg: 0.576:  38%|████▉        | 1920/5000 [12:37<20:14,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.573:  38%|████▉        | 1920/5000 [12:37<20:14,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.573:  39%|█████        | 1940/5000 [12:43<20:04,  2.54it/s]\u001b[A\n",
      "Loss: 0.571 — Avg: 0.569:  39%|█████        | 1940/5000 [12:43<20:04,  2.54it/s]\u001b[A\n",
      "Loss: 0.571 — Avg: 0.569:  39%|█████        | 1960/5000 [12:51<19:55,  2.54it/s]\u001b[A\n",
      "Loss: 0.564 — Avg: 0.569:  39%|█████        | 1960/5000 [12:51<19:55,  2.54it/s]\u001b[A\n",
      "Loss: 0.564 — Avg: 0.569:  40%|█████▏       | 1980/5000 [12:59<19:48,  2.54it/s]\u001b[A\n",
      "Loss: 0.567 — Avg: 0.569:  40%|█████▏       | 1980/5000 [12:59<19:48,  2.54it/s]\u001b[A\n",
      "Loss: 0.567 — Avg: 0.569:  40%|█████▏       | 2000/5000 [13:07<19:41,  2.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [57:35<14:13:44,  1.09s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [57:35<14:13:44,  1.09s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [57:35<14:13:46,  1.09s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "Excere the shoreline, I don't intrude\n",
      "Smmoke a mineAll the olete\n",
      "Said the ocean\n",
      "And I changed my mind (OohLet me off at a listghbel?\n",
      "I sp\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [57:35<14:13:46,  1.09s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▌        | 3160/50000 [57:35<14:13:46,  1.09s/it]\n",
      "Loss: 0.610 — Avg: 0.573:  40%|█████▏       | 2000/5000 [13:07<19:41,  2.54it/s]\u001b[A\n",
      "Loss: 0.610 — Avg: 0.573:  40%|█████▎       | 2020/5000 [13:16<19:35,  2.53it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.571:  40%|█████▎       | 2020/5000 [13:16<19:35,  2.53it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.571:  41%|█████▎       | 2040/5000 [13:26<19:30,  2.53it/s]\u001b[A\n",
      "Loss: 0.522 — Avg: 0.560:  41%|█████▎       | 2040/5000 [13:26<19:30,  2.53it/s]\u001b[A\n",
      "Loss: 0.522 — Avg: 0.560:  41%|█████▎       | 2060/5000 [13:33<19:21,  2.53it/s]\u001b[A\n",
      "Loss: 0.627 — Avg: 0.565:  41%|█████▎       | 2060/5000 [13:33<19:21,  2.53it/s]\u001b[A\n",
      "Loss: 0.627 — Avg: 0.565:  42%|█████▍       | 2080/5000 [13:40<19:12,  2.53it/s]\u001b[A\n",
      "Loss: 0.570 — Avg: 0.570:  42%|█████▍       | 2080/5000 [13:40<19:12,  2.53it/s]\u001b[A\n",
      "Loss: 0.570 — Avg: 0.570:  42%|█████▍       | 2100/5000 [13:48<19:03,  2.54it/s]\u001b[A\n",
      "Loss: 0.534 — Avg: 0.566:  42%|█████▍       | 2100/5000 [13:48<19:03,  2.54it/s]\u001b[A\n",
      "Loss: 0.534 — Avg: 0.566:  42%|█████▌       | 2120/5000 [13:55<18:55,  2.54it/s]\u001b[A\n",
      "Loss: 0.531 — Avg: 0.559:  42%|█████▌       | 2120/5000 [13:55<18:55,  2.54it/s]\u001b[A\n",
      "Loss: 0.531 — Avg: 0.559:  43%|█████▌       | 2140/5000 [14:03<18:47,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.556:  43%|█████▌       | 2140/5000 [14:03<18:47,  2.54it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.556:  43%|█████▌       | 2160/5000 [14:11<18:39,  2.54it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.555:  43%|█████▌       | 2160/5000 [14:11<18:39,  2.54it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.555:  44%|█████▋       | 2180/5000 [14:19<18:31,  2.54it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.595 — Avg: 0.559:  44%|█████▋       | 2180/5000 [14:19<18:31,  2.54it/s]\u001b[A\n",
      "Loss: 0.595 — Avg: 0.559:  44%|█████▋       | 2200/5000 [14:28<18:25,  2.53it/s]\u001b[A\n",
      "Loss: 0.556 — Avg: 0.560:  44%|█████▋       | 2200/5000 [14:28<18:25,  2.53it/s]\u001b[A\n",
      "Loss: 0.556 — Avg: 0.560:  44%|█████▊       | 2220/5000 [14:36<18:18,  2.53it/s]\u001b[A\n",
      "Loss: 0.614 — Avg: 0.567:  44%|█████▊       | 2220/5000 [14:36<18:18,  2.53it/s]\u001b[A\n",
      "Loss: 0.614 — Avg: 0.567:  45%|█████▊       | 2240/5000 [14:44<18:09,  2.53it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.564:  45%|█████▊       | 2240/5000 [14:44<18:09,  2.53it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.564:  45%|█████▉       | 2260/5000 [14:52<18:01,  2.53it/s]\u001b[A\n",
      "Loss: 0.588 — Avg: 0.562:  45%|█████▉       | 2260/5000 [14:52<18:01,  2.53it/s]\u001b[A\n",
      "Loss: 0.588 — Avg: 0.562:  46%|█████▉       | 2280/5000 [15:00<17:54,  2.53it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.564:  46%|█████▉       | 2280/5000 [15:00<17:54,  2.53it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.564:  46%|█████▉       | 2300/5000 [15:07<17:45,  2.53it/s]\u001b[A\n",
      "Loss: 0.566 — Avg: 0.562:  46%|█████▉       | 2300/5000 [15:07<17:45,  2.53it/s]\u001b[A\n",
      "Loss: 0.566 — Avg: 0.562:  46%|██████       | 2320/5000 [15:14<17:35,  2.54it/s]\u001b[A\n",
      "Loss: 0.532 — Avg: 0.560:  46%|██████       | 2320/5000 [15:14<17:35,  2.54it/s]\u001b[A\n",
      "Loss: 0.532 — Avg: 0.560:  47%|██████       | 2340/5000 [15:21<17:27,  2.54it/s]\u001b[A\n",
      "Loss: 0.553 — Avg: 0.556:  47%|██████       | 2340/5000 [15:21<17:27,  2.54it/s]\u001b[A\n",
      "Loss: 0.553 — Avg: 0.556:  47%|██████▏      | 2360/5000 [15:29<17:19,  2.54it/s]\u001b[A\n",
      "Loss: 0.497 — Avg: 0.551:  47%|██████▏      | 2360/5000 [15:29<17:19,  2.54it/s]\u001b[A\n",
      "Loss: 0.497 — Avg: 0.551:  48%|██████▏      | 2380/5000 [15:37<17:11,  2.54it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.547:  48%|██████▏      | 2380/5000 [15:37<17:11,  2.54it/s]\u001b[A\n",
      "Loss: 0.550 — Avg: 0.547:  48%|██████▏      | 2400/5000 [15:44<17:03,  2.54it/s]\u001b[A\n",
      "Loss: 0.552 — Avg: 0.546:  48%|██████▏      | 2400/5000 [15:44<17:03,  2.54it/s]\u001b[A\n",
      "Loss: 0.552 — Avg: 0.546:  48%|██████▎      | 2420/5000 [15:53<16:56,  2.54it/s]\u001b[A\n",
      "Loss: 0.530 — Avg: 0.547:  48%|██████▎      | 2420/5000 [15:53<16:56,  2.54it/s]\u001b[A\n",
      "Loss: 0.530 — Avg: 0.547:  49%|██████▎      | 2440/5000 [16:00<16:48,  2.54it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.542:  49%|██████▎      | 2440/5000 [16:00<16:48,  2.54it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.542:  49%|██████▍      | 2460/5000 [16:08<16:40,  2.54it/s]\u001b[A\n",
      "Loss: 0.553 — Avg: 0.541:  49%|██████▍      | 2460/5000 [16:08<16:40,  2.54it/s]\u001b[A\n",
      "Loss: 0.553 — Avg: 0.541:  50%|██████▍      | 2480/5000 [16:16<16:32,  2.54it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.540:  50%|██████▍      | 2480/5000 [16:16<16:32,  2.54it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.540:  50%|██████▍      | 2480/5000 [16:27<16:43,  2.51it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.540:  50%|██████▌      | 2500/5000 [16:28<16:28,  2.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m2,500 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:00:56<15:03:18,  1.16s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m2,500 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:00:56<15:03:19,  1.16s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:00:56<15:03:20,  1.16s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             , window, angl\n",
      "Laying a sith\n",
      "I hoppeard you're coke and blaced\n",
      "I've been playing a creeWhen I said I wanna listray around on over\n",
      "And I've given all my my somebody's all my\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:00:56<15:03:20,  1.16s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:00:56<15:03:20,  1.16s/it]\n",
      "Loss: 0.525 — Avg: 0.534:  50%|██████▌      | 2500/5000 [16:28<16:28,  2.53it/s]\u001b[A\n",
      "Loss: 0.525 — Avg: 0.534:  50%|██████▌      | 2520/5000 [16:42<16:26,  2.51it/s]\u001b[A\n",
      "Loss: 0.580 — Avg: 0.542:  50%|██████▌      | 2520/5000 [16:42<16:26,  2.51it/s]\u001b[A\n",
      "Loss: 0.580 — Avg: 0.542:  51%|██████▌      | 2540/5000 [16:50<16:19,  2.51it/s]\u001b[A\n",
      "Loss: 0.513 — Avg: 0.542:  51%|██████▌      | 2540/5000 [16:50<16:19,  2.51it/s]\u001b[A\n",
      "Loss: 0.513 — Avg: 0.542:  51%|██████▋      | 2560/5000 [16:58<16:10,  2.51it/s]\u001b[A\n",
      "Loss: 0.545 — Avg: 0.537:  51%|██████▋      | 2560/5000 [16:58<16:10,  2.51it/s]\u001b[A\n",
      "Loss: 0.545 — Avg: 0.537:  52%|██████▋      | 2580/5000 [17:06<16:02,  2.51it/s]\u001b[A\n",
      "Loss: 0.566 — Avg: 0.543:  52%|██████▋      | 2580/5000 [17:06<16:02,  2.51it/s]\u001b[A\n",
      "Loss: 0.566 — Avg: 0.543:  52%|██████▊      | 2600/5000 [17:13<15:54,  2.52it/s]\u001b[A\n",
      "Loss: 0.503 — Avg: 0.539:  52%|██████▊      | 2600/5000 [17:13<15:54,  2.52it/s]\u001b[A\n",
      "Loss: 0.503 — Avg: 0.539:  52%|██████▊      | 2620/5000 [17:22<15:46,  2.51it/s]\u001b[A\n",
      "Loss: 0.608 — Avg: 0.544:  52%|██████▊      | 2620/5000 [17:22<15:46,  2.51it/s]\u001b[A\n",
      "Loss: 0.608 — Avg: 0.544:  53%|██████▊      | 2640/5000 [17:29<15:38,  2.51it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.547:  53%|██████▊      | 2640/5000 [17:29<15:38,  2.51it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.547:  53%|██████▉      | 2660/5000 [17:37<15:30,  2.51it/s]\u001b[A\n",
      "Loss: 0.548 — Avg: 0.545:  53%|██████▉      | 2660/5000 [17:37<15:30,  2.51it/s]\u001b[A\n",
      "Loss: 0.548 — Avg: 0.545:  54%|██████▉      | 2680/5000 [17:45<15:22,  2.51it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.541:  54%|██████▉      | 2680/5000 [17:45<15:22,  2.51it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.541:  54%|███████      | 2700/5000 [17:52<15:13,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.538:  54%|███████      | 2700/5000 [17:52<15:13,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.538:  54%|███████      | 2720/5000 [18:00<15:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.543 — Avg: 0.537:  54%|███████      | 2720/5000 [18:00<15:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.543 — Avg: 0.537:  55%|███████      | 2740/5000 [18:09<14:58,  2.52it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.536:  55%|███████      | 2740/5000 [18:09<14:58,  2.52it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.536:  55%|███████▏     | 2760/5000 [18:16<14:50,  2.52it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.528:  55%|███████▏     | 2760/5000 [18:16<14:50,  2.52it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.528:  56%|███████▏     | 2780/5000 [18:24<14:42,  2.52it/s]\u001b[A\n",
      "Loss: 0.545 — Avg: 0.521:  56%|███████▏     | 2780/5000 [18:24<14:42,  2.52it/s]\u001b[A\n",
      "Loss: 0.545 — Avg: 0.521:  56%|███████▎     | 2800/5000 [18:32<14:34,  2.52it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.526:  56%|███████▎     | 2800/5000 [18:32<14:34,  2.52it/s]\u001b[A\n",
      "Loss: 0.520 — Avg: 0.526:  56%|███████▎     | 2820/5000 [18:40<14:26,  2.52it/s]\u001b[A\n",
      "Loss: 0.512 — Avg: 0.524:  56%|███████▎     | 2820/5000 [18:40<14:26,  2.52it/s]\u001b[A\n",
      "Loss: 0.512 — Avg: 0.524:  57%|███████▍     | 2840/5000 [18:47<14:17,  2.52it/s]\u001b[A\n",
      "Loss: 0.519 — Avg: 0.521:  57%|███████▍     | 2840/5000 [18:47<14:17,  2.52it/s]\u001b[A\n",
      "Loss: 0.519 — Avg: 0.521:  57%|███████▍     | 2860/5000 [18:55<14:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.519:  57%|███████▍     | 2860/5000 [18:55<14:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.519:  58%|███████▍     | 2880/5000 [19:03<14:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.514 — Avg: 0.522:  58%|███████▍     | 2880/5000 [19:03<14:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.514 — Avg: 0.522:  58%|███████▌     | 2900/5000 [19:09<13:52,  2.52it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.519:  58%|███████▌     | 2900/5000 [19:09<13:52,  2.52it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.519:  58%|███████▌     | 2920/5000 [19:16<13:44,  2.52it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.524:  58%|███████▌     | 2920/5000 [19:16<13:44,  2.52it/s]\u001b[A\n",
      "Loss: 0.547 — Avg: 0.524:  59%|███████▋     | 2940/5000 [19:25<13:36,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.520:  59%|███████▋     | 2940/5000 [19:25<13:36,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.520:  59%|███████▋     | 2960/5000 [19:33<13:28,  2.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.477 — Avg: 0.515:  59%|███████▋     | 2960/5000 [19:33<13:28,  2.52it/s]\u001b[A\n",
      "Loss: 0.477 — Avg: 0.515:  60%|███████▋     | 2980/5000 [19:41<13:20,  2.52it/s]\u001b[A\n",
      "Loss: 0.492 — Avg: 0.510:  60%|███████▋     | 2980/5000 [19:41<13:20,  2.52it/s]\u001b[A\n",
      "Loss: 0.492 — Avg: 0.510:  60%|███████▊     | 3000/5000 [19:49<13:12,  2.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:04:17<15:52:53,  1.22s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:04:17<15:52:55,  1.22s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:04:17<15:52:57,  1.22s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             , I don't have to be alone\n",
      "I want to be alone\n",
      "I don't have to be alone\n",
      "Just come on homeThe Moon and stars hang out in bars haunted\n",
      "Over the coast, everyone's convin\n",
      "St the coast\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:04:17<15:52:57,  1.22s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:04:17<15:52:57,  1.22s/it]\n",
      "Loss: 0.525 — Avg: 0.509:  60%|███████▊     | 3000/5000 [19:49<13:12,  2.52it/s]\u001b[A\n",
      "Loss: 0.525 — Avg: 0.509:  60%|███████▊     | 3020/5000 [19:57<13:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.543 — Avg: 0.512:  60%|███████▊     | 3020/5000 [19:57<13:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.543 — Avg: 0.512:  61%|███████▉     | 3040/5000 [20:05<12:57,  2.52it/s]\u001b[A\n",
      "Loss: 0.523 — Avg: 0.518:  61%|███████▉     | 3040/5000 [20:05<12:57,  2.52it/s]\u001b[A\n",
      "Loss: 0.523 — Avg: 0.518:  61%|███████▉     | 3060/5000 [20:12<12:48,  2.52it/s]\u001b[A\n",
      "Loss: 0.473 — Avg: 0.514:  61%|███████▉     | 3060/5000 [20:12<12:48,  2.52it/s]\u001b[A\n",
      "Loss: 0.473 — Avg: 0.514:  62%|████████     | 3080/5000 [20:21<12:41,  2.52it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.508:  62%|████████     | 3080/5000 [20:21<12:41,  2.52it/s]\u001b[A\n",
      "Loss: 0.505 — Avg: 0.508:  62%|████████     | 3100/5000 [20:29<12:33,  2.52it/s]\u001b[A\n",
      "Loss: 0.511 — Avg: 0.508:  62%|████████     | 3100/5000 [20:29<12:33,  2.52it/s]\u001b[A\n",
      "Loss: 0.511 — Avg: 0.508:  62%|████████     | 3120/5000 [20:36<12:25,  2.52it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.516:  62%|████████     | 3120/5000 [20:36<12:25,  2.52it/s]\u001b[A\n",
      "Loss: 0.558 — Avg: 0.516:  63%|████████▏    | 3140/5000 [20:44<12:17,  2.52it/s]\u001b[A\n",
      "Loss: 0.534 — Avg: 0.521:  63%|████████▏    | 3140/5000 [20:44<12:17,  2.52it/s]\u001b[A\n",
      "Loss: 0.534 — Avg: 0.521:  63%|████████▏    | 3160/5000 [20:52<12:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.508 — Avg: 0.519:  63%|████████▏    | 3160/5000 [20:52<12:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.508 — Avg: 0.519:  64%|████████▎    | 3180/5000 [21:00<12:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.518:  64%|████████▎    | 3180/5000 [21:00<12:01,  2.52it/s]\u001b[A\n",
      "Loss: 0.528 — Avg: 0.518:  64%|████████▎    | 3200/5000 [21:08<11:53,  2.52it/s]\u001b[A\n",
      "Loss: 0.548 — Avg: 0.523:  64%|████████▎    | 3200/5000 [21:08<11:53,  2.52it/s]\u001b[A\n",
      "Loss: 0.548 — Avg: 0.523:  64%|████████▎    | 3220/5000 [21:16<11:45,  2.52it/s]\u001b[A\n",
      "Loss: 0.538 — Avg: 0.527:  64%|████████▎    | 3220/5000 [21:16<11:45,  2.52it/s]\u001b[A\n",
      "Loss: 0.538 — Avg: 0.527:  65%|████████▍    | 3240/5000 [21:24<11:37,  2.52it/s]\u001b[A\n",
      "Loss: 0.512 — Avg: 0.527:  65%|████████▍    | 3240/5000 [21:24<11:37,  2.52it/s]\u001b[A\n",
      "Loss: 0.512 — Avg: 0.527:  65%|████████▍    | 3260/5000 [21:33<11:30,  2.52it/s]\u001b[A\n",
      "Loss: 0.531 — Avg: 0.525:  65%|████████▍    | 3260/5000 [21:33<11:30,  2.52it/s]\u001b[A\n",
      "Loss: 0.531 — Avg: 0.525:  66%|████████▌    | 3280/5000 [21:41<11:22,  2.52it/s]\u001b[A\n",
      "Loss: 0.479 — Avg: 0.522:  66%|████████▌    | 3280/5000 [21:41<11:22,  2.52it/s]\u001b[A\n",
      "Loss: 0.479 — Avg: 0.522:  66%|████████▌    | 3300/5000 [21:49<11:14,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.515:  66%|████████▌    | 3300/5000 [21:49<11:14,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.515:  66%|████████▋    | 3320/5000 [21:55<11:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.494 — Avg: 0.513:  66%|████████▋    | 3320/5000 [21:55<11:05,  2.52it/s]\u001b[A\n",
      "Loss: 0.494 — Avg: 0.513:  67%|████████▋    | 3340/5000 [22:03<10:57,  2.52it/s]\u001b[A\n",
      "Loss: 0.522 — Avg: 0.513:  67%|████████▋    | 3340/5000 [22:03<10:57,  2.52it/s]\u001b[A\n",
      "Loss: 0.522 — Avg: 0.513:  67%|████████▋    | 3360/5000 [22:11<10:49,  2.52it/s]\u001b[A\n",
      "Loss: 0.536 — Avg: 0.516:  67%|████████▋    | 3360/5000 [22:11<10:49,  2.52it/s]\u001b[A\n",
      "Loss: 0.536 — Avg: 0.516:  68%|████████▊    | 3380/5000 [22:18<10:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.448 — Avg: 0.510:  68%|████████▊    | 3380/5000 [22:18<10:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.448 — Avg: 0.510:  68%|████████▊    | 3400/5000 [22:26<10:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.486 — Avg: 0.503:  68%|████████▊    | 3400/5000 [22:26<10:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.486 — Avg: 0.503:  68%|████████▉    | 3420/5000 [22:34<10:25,  2.52it/s]\u001b[A\n",
      "Loss: 0.467 — Avg: 0.499:  68%|████████▉    | 3420/5000 [22:34<10:25,  2.52it/s]\u001b[A\n",
      "Loss: 0.467 — Avg: 0.499:  69%|████████▉    | 3440/5000 [22:42<10:18,  2.52it/s]\u001b[A\n",
      "Loss: 0.526 — Avg: 0.497:  69%|████████▉    | 3440/5000 [22:42<10:18,  2.52it/s]\u001b[A\n",
      "Loss: 0.526 — Avg: 0.497:  69%|████████▉    | 3460/5000 [22:50<10:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.469 — Avg: 0.495:  69%|████████▉    | 3460/5000 [22:50<10:09,  2.52it/s]\u001b[A\n",
      "Loss: 0.469 — Avg: 0.495:  70%|█████████    | 3480/5000 [22:58<10:02,  2.52it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.495:  70%|█████████    | 3480/5000 [22:58<10:02,  2.52it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.495:  70%|█████████    | 3500/5000 [23:06<09:54,  2.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m3,500 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:07:34<16:41:36,  1.28s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m3,500 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:07:34<16:41:37,  1.28s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:07:34<16:41:39,  1.28s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "That it going down\n",
      "This isn't have to be alone\n",
      "I want to be wrong\n",
      "Just to be alone, bound\n",
      "It's me like my room\n",
      "But you had if you couldn't open my push start of me in the stulaying\n",
      "But let\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:07:34<16:41:39,  1.28s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:07:34<16:41:39,  1.28s/it]\n",
      "Loss: 0.489 — Avg: 0.493:  70%|█████████    | 3500/5000 [23:06<09:54,  2.52it/s]\u001b[A\n",
      "Loss: 0.489 — Avg: 0.493:  70%|█████████▏   | 3520/5000 [23:15<09:46,  2.52it/s]\u001b[A\n",
      "Loss: 0.492 — Avg: 0.491:  70%|█████████▏   | 3520/5000 [23:15<09:46,  2.52it/s]\u001b[A\n",
      "Loss: 0.492 — Avg: 0.491:  71%|█████████▏   | 3540/5000 [23:23<09:38,  2.52it/s]\u001b[A\n",
      "Loss: 0.504 — Avg: 0.492:  71%|█████████▏   | 3540/5000 [23:23<09:38,  2.52it/s]\u001b[A\n",
      "Loss: 0.504 — Avg: 0.492:  71%|█████████▎   | 3560/5000 [23:33<09:31,  2.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.488 — Avg: 0.495:  71%|█████████▎   | 3560/5000 [23:33<09:31,  2.52it/s]\u001b[A\n",
      "Loss: 0.488 — Avg: 0.495:  72%|█████████▎   | 3580/5000 [23:42<09:24,  2.52it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.498:  72%|█████████▎   | 3580/5000 [23:42<09:24,  2.52it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.498:  72%|█████████▎   | 3600/5000 [23:49<09:16,  2.52it/s]\u001b[A\n",
      "Loss: 0.498 — Avg: 0.499:  72%|█████████▎   | 3600/5000 [23:49<09:16,  2.52it/s]\u001b[A\n",
      "Loss: 0.498 — Avg: 0.499:  72%|█████████▍   | 3620/5000 [23:57<09:07,  2.52it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.501:  72%|█████████▍   | 3620/5000 [23:57<09:07,  2.52it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.501:  73%|█████████▍   | 3640/5000 [24:04<08:59,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.500:  73%|█████████▍   | 3640/5000 [24:04<08:59,  2.52it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.500:  73%|█████████▌   | 3660/5000 [24:11<08:51,  2.52it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.500:  73%|█████████▌   | 3660/5000 [24:11<08:51,  2.52it/s]\u001b[A\n",
      "Loss: 0.521 — Avg: 0.500:  74%|█████████▌   | 3680/5000 [24:18<08:43,  2.52it/s]\u001b[A\n",
      "Loss: 0.466 — Avg: 0.501:  74%|█████████▌   | 3680/5000 [24:18<08:43,  2.52it/s]\u001b[A\n",
      "Loss: 0.466 — Avg: 0.501:  74%|█████████▌   | 3700/5000 [24:26<08:35,  2.52it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.497:  74%|█████████▌   | 3700/5000 [24:26<08:35,  2.52it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.497:  74%|█████████▋   | 3720/5000 [24:34<08:27,  2.52it/s]\u001b[A\n",
      "Loss: 0.443 — Avg: 0.491:  74%|█████████▋   | 3720/5000 [24:34<08:27,  2.52it/s]\u001b[A\n",
      "Loss: 0.443 — Avg: 0.491:  75%|█████████▋   | 3740/5000 [24:42<08:19,  2.52it/s]\u001b[A\n",
      "Loss: 0.454 — Avg: 0.485:  75%|█████████▋   | 3740/5000 [24:42<08:19,  2.52it/s]\u001b[A\n",
      "Loss: 0.454 — Avg: 0.485:  75%|█████████▊   | 3760/5000 [24:48<08:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.509 — Avg: 0.484:  75%|█████████▊   | 3760/5000 [24:48<08:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.509 — Avg: 0.484:  76%|█████████▊   | 3780/5000 [24:56<08:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.483:  76%|█████████▊   | 3780/5000 [24:56<08:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.483:  76%|█████████▉   | 3800/5000 [25:03<07:54,  2.53it/s]\u001b[A\n",
      "Loss: 0.532 — Avg: 0.488:  76%|█████████▉   | 3800/5000 [25:03<07:54,  2.53it/s]\u001b[A\n",
      "Loss: 0.532 — Avg: 0.488:  76%|█████████▉   | 3820/5000 [25:10<07:46,  2.53it/s]\u001b[A\n",
      "Loss: 0.443 — Avg: 0.485:  76%|█████████▉   | 3820/5000 [25:10<07:46,  2.53it/s]\u001b[A\n",
      "Loss: 0.443 — Avg: 0.485:  77%|█████████▉   | 3840/5000 [25:17<07:38,  2.53it/s]\u001b[A\n",
      "Loss: 0.442 — Avg: 0.479:  77%|█████████▉   | 3840/5000 [25:17<07:38,  2.53it/s]\u001b[A\n",
      "Loss: 0.442 — Avg: 0.479:  77%|██████████   | 3860/5000 [25:24<07:30,  2.53it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.476:  77%|██████████   | 3860/5000 [25:24<07:30,  2.53it/s]\u001b[A\n",
      "Loss: 0.500 — Avg: 0.476:  78%|██████████   | 3880/5000 [25:31<07:22,  2.53it/s]\u001b[A\n",
      "Loss: 0.487 — Avg: 0.481:  78%|██████████   | 3880/5000 [25:31<07:22,  2.53it/s]\u001b[A\n",
      "Loss: 0.487 — Avg: 0.481:  78%|██████████▏  | 3900/5000 [25:39<07:14,  2.53it/s]\u001b[A\n",
      "Loss: 0.447 — Avg: 0.479:  78%|██████████▏  | 3900/5000 [25:39<07:14,  2.53it/s]\u001b[A\n",
      "Loss: 0.447 — Avg: 0.479:  78%|██████████▏  | 3920/5000 [25:46<07:06,  2.53it/s]\u001b[A\n",
      "Loss: 0.481 — Avg: 0.477:  78%|██████████▏  | 3920/5000 [25:46<07:06,  2.53it/s]\u001b[A\n",
      "Loss: 0.481 — Avg: 0.477:  79%|██████████▏  | 3940/5000 [25:54<06:58,  2.53it/s]\u001b[A\n",
      "Loss: 0.421 — Avg: 0.469:  79%|██████████▏  | 3940/5000 [25:54<06:58,  2.53it/s]\u001b[A\n",
      "Loss: 0.421 — Avg: 0.469:  79%|██████████▎  | 3960/5000 [26:02<06:50,  2.53it/s]\u001b[A\n",
      "Loss: 0.487 — Avg: 0.468:  79%|██████████▎  | 3960/5000 [26:02<06:50,  2.53it/s]\u001b[A\n",
      "Loss: 0.487 — Avg: 0.468:  80%|██████████▎  | 3980/5000 [26:11<06:42,  2.53it/s]\u001b[A\n",
      "Loss: 0.445 — Avg: 0.466:  80%|██████████▎  | 3980/5000 [26:11<06:42,  2.53it/s]\u001b[A\n",
      "Loss: 0.445 — Avg: 0.466:  80%|██████████▍  | 4000/5000 [26:18<06:34,  2.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:10:47<17:29:13,  1.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:10:47<17:29:14,  1.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:10:47<17:29:15,  1.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "But I'm screaming there with my life ofom\n",
      "Lay around the Elvangelir\n",
      "That's no, you don't be alone\n",
      "I want to be lay aroundAnd I'll leave it's Christmas\n",
      "Let the last time for now\n",
      "I feel something when I see you\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:10:47<17:29:15,  1.34s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:10:47<17:29:15,  1.34s/it]\n",
      "Loss: 0.443 — Avg: 0.465:  80%|██████████▍  | 4000/5000 [26:19<06:34,  2.53it/s]\u001b[A\n",
      "Loss: 0.443 — Avg: 0.465:  80%|██████████▍  | 4020/5000 [26:27<06:27,  2.53it/s]\u001b[A\n",
      "Loss: 0.474 — Avg: 0.464:  80%|██████████▍  | 4020/5000 [26:27<06:27,  2.53it/s]\u001b[A\n",
      "Loss: 0.474 — Avg: 0.464:  81%|██████████▌  | 4040/5000 [26:34<06:18,  2.53it/s]\u001b[A\n",
      "Loss: 0.456 — Avg: 0.463:  81%|██████████▌  | 4040/5000 [26:34<06:18,  2.53it/s]\u001b[A\n",
      "Loss: 0.456 — Avg: 0.463:  81%|██████████▌  | 4060/5000 [26:43<06:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.468 — Avg: 0.464:  81%|██████████▌  | 4060/5000 [26:43<06:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.468 — Avg: 0.464:  82%|██████████▌  | 4080/5000 [26:52<06:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.471 — Avg: 0.468:  82%|██████████▌  | 4080/5000 [26:52<06:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.471 — Avg: 0.468:  82%|██████████▋  | 4100/5000 [27:00<05:55,  2.53it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.466:  82%|██████████▋  | 4100/5000 [27:00<05:55,  2.53it/s]\u001b[A\n",
      "Loss: 0.499 — Avg: 0.466:  82%|██████████▋  | 4120/5000 [27:08<05:47,  2.53it/s]\u001b[A\n",
      "Loss: 0.457 — Avg: 0.469:  82%|██████████▋  | 4120/5000 [27:08<05:47,  2.53it/s]\u001b[A\n",
      "Loss: 0.457 — Avg: 0.469:  83%|██████████▊  | 4140/5000 [27:16<05:39,  2.53it/s]\u001b[A\n",
      "Loss: 0.448 — Avg: 0.464:  83%|██████████▊  | 4140/5000 [27:16<05:39,  2.53it/s]\u001b[A\n",
      "Loss: 0.448 — Avg: 0.464:  83%|██████████▊  | 4160/5000 [27:24<05:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.473 — Avg: 0.467:  83%|██████████▊  | 4160/5000 [27:24<05:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.473 — Avg: 0.467:  84%|██████████▊  | 4180/5000 [27:31<05:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.456 — Avg: 0.464:  84%|██████████▊  | 4180/5000 [27:31<05:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.456 — Avg: 0.464:  84%|██████████▉  | 4200/5000 [27:38<05:15,  2.53it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.469:  84%|██████████▉  | 4200/5000 [27:39<05:16,  2.53it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.469:  84%|██████████▉  | 4220/5000 [27:47<05:08,  2.53it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.473:  84%|██████████▉  | 4220/5000 [27:47<05:08,  2.53it/s]\u001b[A\n",
      "Loss: 0.524 — Avg: 0.473:  85%|███████████  | 4240/5000 [27:54<05:00,  2.53it/s]\u001b[A\n",
      "Loss: 0.452 — Avg: 0.475:  85%|███████████  | 4240/5000 [27:54<05:00,  2.53it/s]\u001b[A\n",
      "Loss: 0.452 — Avg: 0.475:  85%|███████████  | 4260/5000 [28:02<04:52,  2.53it/s]\u001b[A\n",
      "Loss: 0.463 — Avg: 0.474:  85%|███████████  | 4260/5000 [28:02<04:52,  2.53it/s]\u001b[A\n",
      "Loss: 0.463 — Avg: 0.474:  86%|███████████▏ | 4280/5000 [28:10<04:44,  2.53it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.473:  86%|███████████▏ | 4280/5000 [28:10<04:44,  2.53it/s]\u001b[A\n",
      "Loss: 0.491 — Avg: 0.473:  86%|███████████▏ | 4300/5000 [28:18<04:36,  2.53it/s]\u001b[A\n",
      "Loss: 0.460 — Avg: 0.474:  86%|███████████▏ | 4300/5000 [28:18<04:36,  2.53it/s]\u001b[A\n",
      "Loss: 0.460 — Avg: 0.474:  86%|███████████▏ | 4320/5000 [28:25<04:28,  2.53it/s]\u001b[A\n",
      "Loss: 0.452 — Avg: 0.469:  86%|███████████▏ | 4320/5000 [28:25<04:28,  2.53it/s]\u001b[A\n",
      "Loss: 0.452 — Avg: 0.469:  87%|███████████▎ | 4340/5000 [28:34<04:20,  2.53it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.476 — Avg: 0.471:  87%|███████████▎ | 4340/5000 [28:34<04:20,  2.53it/s]\u001b[A\n",
      "Loss: 0.476 — Avg: 0.471:  87%|███████████▎ | 4360/5000 [28:41<04:12,  2.53it/s]\u001b[A\n",
      "Loss: 0.407 — Avg: 0.463:  87%|███████████▎ | 4360/5000 [28:41<04:12,  2.53it/s]\u001b[A\n",
      "Loss: 0.407 — Avg: 0.463:  88%|███████████▍ | 4380/5000 [28:50<04:04,  2.53it/s]\u001b[A\n",
      "Loss: 0.453 — Avg: 0.458:  88%|███████████▍ | 4380/5000 [28:50<04:04,  2.53it/s]\u001b[A\n",
      "Loss: 0.453 — Avg: 0.458:  88%|███████████▍ | 4400/5000 [28:58<03:57,  2.53it/s]\u001b[A\n",
      "Loss: 0.471 — Avg: 0.462:  88%|███████████▍ | 4400/5000 [28:58<03:57,  2.53it/s]\u001b[A\n",
      "Loss: 0.471 — Avg: 0.462:  88%|███████████▍ | 4420/5000 [29:06<03:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.462 — Avg: 0.460:  88%|███████████▍ | 4420/5000 [29:06<03:49,  2.53it/s]\u001b[A\n",
      "Loss: 0.462 — Avg: 0.460:  89%|███████████▌ | 4440/5000 [29:13<03:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.457 — Avg: 0.460:  89%|███████████▌ | 4440/5000 [29:13<03:41,  2.53it/s]\u001b[A\n",
      "Loss: 0.457 — Avg: 0.460:  89%|███████████▌ | 4460/5000 [29:20<03:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.450 — Avg: 0.458:  89%|███████████▌ | 4460/5000 [29:20<03:33,  2.53it/s]\u001b[A\n",
      "Loss: 0.450 — Avg: 0.458:  90%|███████████▋ | 4480/5000 [29:28<03:25,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.458:  90%|███████████▋ | 4480/5000 [29:28<03:25,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.458:  90%|███████████▋ | 4500/5000 [29:36<03:17,  2.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m4,500 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:14:04<18:17:56,  1.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m4,500 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:14:04<18:17:56,  1.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:14:04<18:17:58,  1.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "But I'm quian is bullhead near, something to loby\n",
      "I turned around, but I love a danger\n",
      "You're a brother to me\n",
      "You're my partner in cries, I'm feeling finePart of me wants you\n",
      "But\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:14:04<18:17:58,  1.41s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:14:04<18:17:58,  1.41s/it]\n",
      "Loss: 0.459 — Avg: 0.460:  90%|███████████▋ | 4500/5000 [29:36<03:17,  2.53it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.460:  90%|███████████▊ | 4520/5000 [29:43<03:09,  2.53it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.460:  90%|███████████▊ | 4520/5000 [29:43<03:09,  2.53it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.460:  91%|███████████▊ | 4540/5000 [29:50<03:01,  2.54it/s]\u001b[A\n",
      "Loss: 0.494 — Avg: 0.465:  91%|███████████▊ | 4540/5000 [29:50<03:01,  2.54it/s]\u001b[A\n",
      "Loss: 0.494 — Avg: 0.465:  91%|███████████▊ | 4560/5000 [29:57<02:53,  2.54it/s]\u001b[A\n",
      "Loss: 0.468 — Avg: 0.467:  91%|███████████▊ | 4560/5000 [29:57<02:53,  2.54it/s]\u001b[A\n",
      "Loss: 0.468 — Avg: 0.467:  92%|███████████▉ | 4580/5000 [30:06<02:45,  2.54it/s]\u001b[A\n",
      "Loss: 0.498 — Avg: 0.470:  92%|███████████▉ | 4580/5000 [30:06<02:45,  2.54it/s]\u001b[A\n",
      "Loss: 0.498 — Avg: 0.470:  92%|███████████▉ | 4600/5000 [30:13<02:37,  2.54it/s]\u001b[A\n",
      "Loss: 0.458 — Avg: 0.471:  92%|███████████▉ | 4600/5000 [30:13<02:37,  2.54it/s]\u001b[A\n",
      "Loss: 0.458 — Avg: 0.471:  92%|████████████ | 4620/5000 [30:21<02:29,  2.54it/s]\u001b[A\n",
      "Loss: 0.449 — Avg: 0.468:  92%|████████████ | 4620/5000 [30:21<02:29,  2.54it/s]\u001b[A\n",
      "Loss: 0.449 — Avg: 0.468:  93%|████████████ | 4640/5000 [30:28<02:21,  2.54it/s]\u001b[A\n",
      "Loss: 0.431 — Avg: 0.463:  93%|████████████ | 4640/5000 [30:28<02:21,  2.54it/s]\u001b[A\n",
      "Loss: 0.431 — Avg: 0.463:  93%|████████████ | 4660/5000 [30:36<02:14,  2.54it/s]\u001b[A\n",
      "Loss: 0.455 — Avg: 0.459:  93%|████████████ | 4660/5000 [30:36<02:14,  2.54it/s]\u001b[A\n",
      "Loss: 0.455 — Avg: 0.459:  94%|████████████▏| 4680/5000 [30:45<02:06,  2.54it/s]\u001b[A\n",
      "Loss: 0.466 — Avg: 0.458:  94%|████████████▏| 4680/5000 [30:45<02:06,  2.54it/s]\u001b[A\n",
      "Loss: 0.466 — Avg: 0.458:  94%|████████████▏| 4700/5000 [30:54<01:58,  2.53it/s]\u001b[A\n",
      "Loss: 0.453 — Avg: 0.461:  94%|████████████▏| 4700/5000 [30:54<01:58,  2.53it/s]\u001b[A\n",
      "Loss: 0.453 — Avg: 0.461:  94%|████████████▎| 4720/5000 [31:04<01:50,  2.53it/s]\u001b[A\n",
      "Loss: 0.440 — Avg: 0.457:  94%|████████████▎| 4720/5000 [31:04<01:50,  2.53it/s]\u001b[A\n",
      "Loss: 0.440 — Avg: 0.457:  95%|████████████▎| 4740/5000 [31:13<01:42,  2.53it/s]\u001b[A\n",
      "Loss: 0.410 — Avg: 0.451:  95%|████████████▎| 4740/5000 [31:13<01:42,  2.53it/s]\u001b[A\n",
      "Loss: 0.410 — Avg: 0.451:  95%|████████████▍| 4760/5000 [31:20<01:34,  2.53it/s]\u001b[A\n",
      "Loss: 0.451 — Avg: 0.449:  95%|████████████▍| 4760/5000 [31:20<01:34,  2.53it/s]\u001b[A\n",
      "Loss: 0.451 — Avg: 0.449:  96%|████████████▍| 4780/5000 [31:30<01:27,  2.53it/s]\u001b[A\n",
      "Loss: 0.451 — Avg: 0.447:  96%|████████████▍| 4780/5000 [31:30<01:27,  2.53it/s]\u001b[A\n",
      "Loss: 0.451 — Avg: 0.447:  96%|████████████▍| 4800/5000 [31:38<01:19,  2.53it/s]\u001b[A\n",
      "Loss: 0.439 — Avg: 0.448:  96%|████████████▍| 4800/5000 [31:38<01:19,  2.53it/s]\u001b[A\n",
      "Loss: 0.439 — Avg: 0.448:  96%|████████████▌| 4820/5000 [31:46<01:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.478 — Avg: 0.450:  96%|████████████▌| 4820/5000 [31:46<01:11,  2.53it/s]\u001b[A\n",
      "Loss: 0.478 — Avg: 0.450:  97%|████████████▌| 4840/5000 [31:53<01:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.477 — Avg: 0.455:  97%|████████████▌| 4840/5000 [31:53<01:03,  2.53it/s]\u001b[A\n",
      "Loss: 0.477 — Avg: 0.455:  97%|████████████▋| 4860/5000 [32:00<00:55,  2.53it/s]\u001b[A\n",
      "Loss: 0.446 — Avg: 0.456:  97%|████████████▋| 4860/5000 [32:00<00:55,  2.53it/s]\u001b[A\n",
      "Loss: 0.446 — Avg: 0.456:  98%|████████████▋| 4880/5000 [32:08<00:47,  2.53it/s]\u001b[A\n",
      "Loss: 0.460 — Avg: 0.456:  98%|████████████▋| 4880/5000 [32:08<00:47,  2.53it/s]\u001b[A\n",
      "Loss: 0.460 — Avg: 0.456:  98%|████████████▋| 4900/5000 [32:16<00:39,  2.53it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.457:  98%|████████████▋| 4900/5000 [32:16<00:39,  2.53it/s]\u001b[A\n",
      "Loss: 0.459 — Avg: 0.457:  98%|████████████▊| 4920/5000 [32:25<00:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.454 — Avg: 0.456:  98%|████████████▊| 4920/5000 [32:25<00:31,  2.53it/s]\u001b[A\n",
      "Loss: 0.454 — Avg: 0.456:  99%|████████████▊| 4940/5000 [32:34<00:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.444 — Avg: 0.454:  99%|████████████▊| 4940/5000 [32:34<00:23,  2.53it/s]\u001b[A\n",
      "Loss: 0.444 — Avg: 0.454:  99%|████████████▉| 4960/5000 [32:42<00:15,  2.53it/s]\u001b[A\n",
      "Loss: 0.463 — Avg: 0.455:  99%|████████████▉| 4960/5000 [32:42<00:15,  2.53it/s]\u001b[A\n",
      "Loss: 0.463 — Avg: 0.455: 100%|████████████▉| 4980/5000 [32:49<00:07,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.457: 100%|████████████▉| 4980/5000 [32:50<00:07,  2.53it/s]\u001b[A\n",
      "Loss: 0.470 — Avg: 0.457: 100%|█████████████| 5000/5000 [32:57<00:00,  2.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:17:25<19:07:42,  1.47s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:17:25<19:07:42,  1.47s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:17:25<19:07:43,  1.47s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             \n",
      "And nothing else mattersNever cared for what they say\n",
      "Never cared for what they know\n",
      "But I knowNever opened myself this way\n",
      "Life is ours, we live it ours, we live live it our way\n",
      "All these words I don't just say\n",
      "And nothing else mattersT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:17:25<19:07:43,  1.47s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             ==========\n",
      "Loss: 0.578 — Avg: 0.613:   6%|▍      | 3160/50000 [1:17:25<19:07:43,  1.47s/it]\n",
      "Loss: 0.419 — Avg: 0.454: 100%|█████████████| 5000/5000 [32:57<00:00,  2.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.419 — Avg: 0.454: 100%|█████████████| 5000/5000 [32:57<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ai.train(data, batch_size=4, num_steps=5000, generate_every=500, save_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dde80e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mlove \u001b[0mWhile my snow alongI 7-odilehite a stublellin'\n",
      "The drones, we's too are oneinets are\n",
      "To the car\n",
      "And you're the mornin' tears my muscles\n",
      "When\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mrand pled up with a wave that pairI don't still love that that lifts you up\n",
      "I hope you kiss\n",
      "My rotten head\n",
      "And pull the plug\n",
      "Know that I've burned every\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mown Wisitation with the sister didnfuin'\n",
      "Howow To some Watching the sad my friends are all wild reedsWishing?\n",
      "The light you ratfishing?\n",
      "The you're not\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mraking leave was your wornile you more to tell her mouth\n",
      "You're not my waitingNext time you're getting so dangerYou're a brother to me\n",
      "You're my partner in crime\n",
      "You're the feeling I get\n",
      "When youWhen you're\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mTo floatbed\n",
      "Y'all lack humanity, drowning in vanity\n",
      "Craving humanity\n",
      "Y'all lack humanity, I need vanity\n",
      "Craving humanity\n",
      "I need(Thoatfrase who have fors, drowning in v\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mraviadiut in my must began sets\n",
      "After a while you went quiet and I got mean\n",
      "Always pushing you away from me\n",
      "But you come back with gravity\n",
      "And when I call you come home\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mrapist's groundAnd I've gen in the yarden waller than me\n",
      "Robing, and get back to gold was the shot down by the thing\n",
      "Robotel, so ten\n",
      "Robot got food on\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mrap country for heavene thoseless in my money and I I hide\n",
      "Every and I eary and I earned\n",
      "When I'm lonely, that's when I'll burn itDo you feel ashamed\n",
      "\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mraviiolet cover me up\n",
      "Went looking for a creation myth\n",
      "Ended up with a pairsted\n",
      "Emberediced for musict, we'll dying for blood\n",
      "I'm los and tired of\n",
      "==========\n",
      "\u001b[1mlove \u001b[0mWatcching the sitting down\n",
      "I used to fullfair of cracked lips\n",
      "Windows down, scream along\n",
      "To some AmerictuckH bad watching the starmericft raviceme\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"love \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b265526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
